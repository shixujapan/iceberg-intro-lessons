{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254b6cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "NESSIE_URI = 'http://nessie:19120/api/v1'\n",
    "MINIO_ACCESS_KEY = '1LuxaJ1ixMxGn5qow5x2'\n",
    "MINIO_SECRET_KEY = 'RofDM8mka10p0KGgwsTMKBXlJMCuRVdoAlFjN3Ah'\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster(\"local[4]\")\n",
    "        .setAppName('app_name')\n",
    "        .set(\"spark.driver.memory\", \"2g\")\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.67.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178,org.apache.spark:spark-avro_2.12:3.5.0')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.warehouse', 's3a://warehouse')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', 'http://minio:9000')\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.hadoop.fs.s3a.access.key', MINIO_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', MINIO_SECRET_KEY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18327b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/docker/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/docker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/docker/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.3_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c7d28123-852f-48ca-92fd-4a52c22d0b23;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.3.1 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.3_2.12;0.67.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.5.0 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.5.0/spark-avro_2.12-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.5.0!spark-avro_2.12.jar (413ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (386ms)\n",
      ":: resolution report :: resolve 14770ms :: artifacts dl 1108ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.5.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.3_2.12;0.67.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   2   |   2   |   0   ||   13  |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c7d28123-852f-48ca-92fd-4a52c22d0b23\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 11 already retrieved (314kB/185ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/19 13:09:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/19 13:09:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/19 13:09:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee44e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/19 13:11:32 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://warehouse/df_open_2023_684454b3-b932-4cfe-bed8-dea532111106/metadata/304974a8-cdf3-4651-9856-f8fbe5910c40-m0.avro.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://warehouse/df_open_2023_684454b3-b932-4cfe-bed8-dea532111106/metadata/304974a8-cdf3-4651-9856-f8fbe5910c40-m0.avro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:177\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.load.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"avro\").load(\"s3a://warehouse/df_open_2023_684454b3-b932-4cfe-bed8-dea532111106/metadata/304974a8-cdf3-4651-9856-f8fbe5910c40-m0.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3510af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../datasets/df_open_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53a5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.createOrReplaceTempView(\"csv_open_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f993e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Records should be preordered by partition column\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS nessie.df_open_2023_lesson3_3 USING iceberg PARTITIONED BY (regionName) AS SELECT * FROM csv_open_2023 ORDER BY regionName;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1638191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+\n",
      "|col_name           |data_type |comment|\n",
      "+-------------------+----------+-------+\n",
      "|competitorId       |string    |       |\n",
      "|competitorName     |string    |       |\n",
      "|firstName          |string    |       |\n",
      "|lastName           |string    |       |\n",
      "|status             |string    |       |\n",
      "|gender             |string    |       |\n",
      "|countryOfOriginCode|string    |       |\n",
      "|countryOfOriginName|string    |       |\n",
      "|regionId           |string    |       |\n",
      "|regionName         |string    |       |\n",
      "|affiliateId        |string    |       |\n",
      "|affiliateName      |string    |       |\n",
      "|age                |string    |       |\n",
      "|height             |string    |       |\n",
      "|weight             |string    |       |\n",
      "|overallRank        |string    |       |\n",
      "|overallScore       |string    |       |\n",
      "|genderId           |string    |       |\n",
      "|year               |string    |       |\n",
      "|                   |          |       |\n",
      "|# Partitioning     |          |       |\n",
      "|Part 0             |regionName|       |\n",
      "+-------------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE nessie.df_open_2023_lesson3_3\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242df7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE nessie.df_open_2023_lesson3 (\\n  competitorId STRING,\\n  competitorName STRING,\\n  firstName STRING,\\n  lastName STRING,\\n  status STRING,\\n  gender STRING,\\n  countryOfOriginCode STRING,\\n  countryOfOriginName STRING,\\n  regionId STRING,\\n  regionName STRING,\\n  affiliateId STRING,\\n  affiliateName STRING,\\n  age STRING,\\n  height STRING,\\n  weight STRING,\\n  overallRank STRING,\\n  overallScore STRING,\\n  genderId STRING,\\n  year STRING)\\nUSING iceberg\\nPARTITIONED BY (regionName)\\nLOCATION 's3a://warehouse/df_open_2023_lesson3_c8e6b4ad-58bc-43f7-9656-2d63e3d74ac0'\\nTBLPROPERTIES (\\n  'current-snapshot-id' = '3622300926360899964',\\n  'format' = 'iceberg/parquet',\\n  'format-version' = '1',\\n  'gc.enabled' = 'false',\\n  'nessie.commit.id' = '7f01a941e17bf2b5b4f13a08d579fee9e8750a250fd2e6d1789efbbc78e555da')\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE nessie.df_open_2023_lesson3\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "706aad53",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot add duplicate partition field null=truncate[1](ref(name=\"firstName\")), conflicts with 1001: firstName_trunc_1: truncate[1](3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mALTER TABLE nessie.df_open_2023_lesson3 ADD PARTITION FIELD truncate(1, firstName)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot add duplicate partition field null=truncate[1](ref(name=\"firstName\")), conflicts with 1001: firstName_trunc_1: truncate[1](3)"
     ]
    }
   ],
   "source": [
    "# truncate(1, firstName) is a hidden partitioning\n",
    "spark.sql(\"ALTER TABLE nessie.df_open_2023_lesson3 ADD PARTITION FIELD truncate(1, firstName)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6777477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+-------+\n",
      "|col_name           |data_type             |comment|\n",
      "+-------------------+----------------------+-------+\n",
      "|competitorId       |string                |       |\n",
      "|competitorName     |string                |       |\n",
      "|firstName          |string                |       |\n",
      "|lastName           |string                |       |\n",
      "|status             |string                |       |\n",
      "|gender             |string                |       |\n",
      "|countryOfOriginCode|string                |       |\n",
      "|countryOfOriginName|string                |       |\n",
      "|regionId           |string                |       |\n",
      "|regionName         |string                |       |\n",
      "|affiliateId        |string                |       |\n",
      "|affiliateName      |string                |       |\n",
      "|age                |string                |       |\n",
      "|height             |string                |       |\n",
      "|weight             |string                |       |\n",
      "|overallRank        |string                |       |\n",
      "|overallScore       |string                |       |\n",
      "|genderId           |string                |       |\n",
      "|year               |string                |       |\n",
      "|                   |                      |       |\n",
      "|# Partitioning     |                      |       |\n",
      "|Part 0             |regionName            |       |\n",
      "|Part 1             |truncate(1, firstName)|       |\n",
      "+-------------------+----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE nessie.df_open_2023_lesson3\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceb4b0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|competitorName |firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|regionName|affiliateId|affiliateName     |age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+------------------+---+------+------+-----------+------------+--------+----+\n",
      "|1515387     |Darren Zurnamer|Darren   |Zurnamer|ACT   |M     |ZA                 |South Africa       |30      |Africa    |17025      |Cape CrossFit 7130|22 |172 cm|82 kg |21         |1023        |1       |2023|\n",
      "|101860      |Alan Foulis    |Alan     |Foulis  |ACT   |M     |ZA                 |South Africa       |30      |Africa    |31119      |CrossFit FBDV     |37 |176 cm|190 lb|135        |2829        |18      |2023|\n",
      "|901467      |Kealan Henry   |Kealan   |Henry   |ACT   |M     |ZA                 |South Africa       |30      |Africa    |29491      |CrossFit HFS      |30 |168 cm|80 kg |164        |3212        |1       |2023|\n",
      "|1828615     |Daniel Griesel |Daniel   |Griesel |ACT   |M     |ZA                 |South Africa       |30      |Africa    |None       |null              |27 |null  |null  |241        |4107        |1       |2023|\n",
      "|1624460     |Driss Bouchiah |Driss    |Bouchiah|ACT   |M     |MA                 |Morocco            |30      |Africa    |31444      |DLK CrossFit      |27 |178 cm|168 lb|695        |8817        |1       |2023|\n",
      "+------------+---------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.df_open_2023_lesson3\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b58f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|\n",
      "+--------------------------+----------------------+---------------------+\n",
      "|                         7|                   551|              8998481|\n",
      "+--------------------------+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CALL nessie.system.rewrite_data_files('df_open_2023_lesson3')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b087c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------+-----------+---------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|competitorName  |firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|regionName   |affiliateId|affiliateName        |age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------+-----------+---------------------+---+------+------+-----------+------------+--------+----+\n",
      "|2125703     |기태 박         |기태     |박      |ACT   |M     |KR                 |Korea, Republic of |28      |Asia         |23370      |CrossFit Rikka       |23 |null  |null  |10135      |65453       |1       |2023|\n",
      "|2474336     |기훈 김         |기훈     |김      |ACT   |M     |KR                 |Korea, Republic of |28      |Asia         |None       |null                 |30 |null  |null  |51022      |225030      |1       |2023|\n",
      "|478414      |Agustin Richelme|Agustin  |Richelme|ACT   |M     |AR                 |Argentina          |33      |South America|None       |null                 |26 |173 cm|85 kg |46         |1631        |1       |2023|\n",
      "|889151      |Andres Escobar  |Andres   |Escobar |ACT   |M     |CL                 |Chile              |33      |South America|29734      |CrossFit Gorilla     |38 |71 in |206 lb|366        |5478        |18      |2023|\n",
      "|169360      |Anderon Primo   |Anderon  |Primo   |ACT   |M     |BR                 |Brazil             |33      |South America|20268      |CrossFit Bauru Hangar|28 |180 cm|200 lb|390        |5767        |1       |2023|\n",
      "+------------+----------------+---------+--------+------+------+-------------------+-------------------+--------+-------------+-----------+---------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.df_open_2023_lesson3\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a60799a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+-------+\n",
      "|col_name           |data_type             |comment|\n",
      "+-------------------+----------------------+-------+\n",
      "|competitorId       |string                |       |\n",
      "|competitorName     |string                |       |\n",
      "|firstName          |string                |       |\n",
      "|lastName           |string                |       |\n",
      "|status             |string                |       |\n",
      "|gender             |string                |       |\n",
      "|countryOfOriginCode|string                |       |\n",
      "|countryOfOriginName|string                |       |\n",
      "|regionId           |string                |       |\n",
      "|regionName         |string                |       |\n",
      "|affiliateId        |string                |       |\n",
      "|affiliateName      |string                |       |\n",
      "|age                |string                |       |\n",
      "|height             |string                |       |\n",
      "|weight             |string                |       |\n",
      "|overallRank        |string                |       |\n",
      "|overallScore       |string                |       |\n",
      "|genderId           |string                |       |\n",
      "|year               |string                |       |\n",
      "|                   |                      |       |\n",
      "|# Partitioning     |                      |       |\n",
      "|Part 0             |regionName            |       |\n",
      "|Part 1             |truncate(1, firstName)|       |\n",
      "+-------------------+----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE nessie.df_open_2023_lesson3\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a8efb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+----------------------+---+------+------+-----------+------------+--------+----+\n",
      "|competitorId|competitorName   |firstName|lastName|status|gender|countryOfOriginCode|countryOfOriginName|regionId|regionName|affiliateId|affiliateName         |age|height|weight|overallRank|overallScore|genderId|year|\n",
      "+------------+-----------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+----------------------+---+------+------+-----------+------------+--------+----+\n",
      "|2125703     |기태 박          |기태     |박      |ACT   |M     |KR                 |Korea, Republic of |28      |Asia      |23370      |CrossFit Rikka        |23 |null  |null  |10135      |65453       |1       |2023|\n",
      "|2474336     |기훈 김          |기훈     |김      |ACT   |M     |KR                 |Korea, Republic of |28      |Asia      |None       |null                  |30 |null  |null  |51022      |225030      |1       |2023|\n",
      "|2511943     |Yurii Bulat      |Yurii    |Bulat   |ACT   |M     |RU                 |null               |28      |Asia      |None       |null                  |28 |null  |null  |231        |4021        |1       |2023|\n",
      "|498668      |Yasuhiro Uchibori|Yasuhiro |Uchibori|ACT   |M     |JP                 |Japan              |28      |Asia      |24943      |CrossFit Motomachi Bay|35 |170 cm|80 kg |237        |4080        |18      |2023|\n",
      "|896059      |Yu-sen Zhu       |Yu-sen   |Zhu     |ACT   |M     |TW                 |Chinese Taipei     |28      |Asia      |None       |null                  |26 |null  |null  |803        |9824        |1       |2023|\n",
      "|912071      |YiLiang Liu      |YiLiang  |Liu     |ACT   |M     |CN                 |China              |28      |Asia      |17777      |Stud CrossFit         |30 |176 cm|87 kg |1172       |13089       |1       |2023|\n",
      "|473943      |YeonGyu Choo     |YeonGyu  |Choo    |ACT   |M     |KR                 |Korea, Republic of |28      |Asia      |30046      |Team Nomad CrossFit   |30 |174 cm|76 kg |1738       |17170       |1       |2023|\n",
      "|2070222     |Yunseop Lim      |Yunseop  |Lim     |ACT   |M     |KR                 |Korea, Republic of |28      |Asia      |29913      |CrossFit Arcade       |22 |null  |null  |1922       |18604       |1       |2023|\n",
      "|1178552     |Youngsan Choi    |Youngsan |Choi    |ACT   |M     |KR                 |Korea, Republic of |28      |Asia      |17675      |CrossFit MATE         |29 |null  |165 lb|2011       |19359       |1       |2023|\n",
      "|702579      |Youssef Dagher   |Youssef  |Dagher  |ACT   |M     |LB                 |Lebanon            |28      |Asia      |20043      |INRI CrossFit         |42 |180 cm|93 kg |2515       |23047       |12      |2023|\n",
      "+------------+-----------------+---------+--------+------+------+-------------------+-------------------+--------+----------+-----------+----------------------+---+------+------+-----------+------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM nessie.df_open_2023_lesson3 WHERE regionName = 'Asia'\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c7159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
